# WikiPhilosophy
There's an old internet adage that, if you click on the first non-parenthesized, non-italicized link on a Wikipedia page, ignoring external links or links that don't lead to existing pages (red links), you'll eventually end up on the Wikipedia page for [Philosophy](https://en.wikipedia.org/wiki/Philosophy). In fact, there's even a [Wikipedia page](https://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy) describing this exact game!

While I know it's not proper "GitHub etiquette" to write a long-winded README, but since this is just a side-project that I don't intend to scale beyond my own entertainment, I'm going to use this file as a way to document my process.

Given the nature of the method, I wanted to try my hand at coding a way to automatically compute the path from a given page to "Philosophy", if it exists. However, I wanted to take it one step further and preprocess everything. While Wikipedia is an everchanging database of information, constantly being updated, with new pages being created and deleted, new links being added or removed, I wanted to at least make an effort to capture the results at one given time (which turned out to be between December 9 to December 15). As such, the "cache" folder of this project is representative of Wikipedia articles of that timespan. I would love to have it consistently update with the correct information, but parsing over six million Wikipedia articles on a ThinkPad laptop isn't exactly *efficient*, even using multiprocessing. In fact, it took me about a day to retrieve a list of all the articles, and over a week to get the first link they point to!

## Methodology
The first step of this process was finding a consisent way to get a list of all existing English Wikipedia articles. This was difficult at first, but fortunately there is a Wikipedia page for [all English articles](https://en.wikipedia.org/w/index.php?title=Special:AllPages). Each page contains about 200 or so links, with some being redirects (those in italics). Unfortunately for me, I could not use multiprocessing to scrape these pages, as the links that show on the next page are dependent on the current page (evidenced by the nature of the title, adding the extension `&from="page"`). While the exact amount of pages on Wikipedia differs from day to day, I had scraped it when there were 6,583,099 articles. The actual number for this project is likely smaller, as some articles that had been listed no longer exist, and newer articles were not added.

To store these articles in the most useful way, I kept them in a **dictionary** format, with the key being the title of the article, and the value being the link to the article. When this process was completed, I exported the result in JSON format to a file called `articles.json`. This file is not visible in the repository, as the resulting JSON was about **533 MB large**, and GitHub has a 100 MB file upload limit. Thus, I broke the file into 28 separate JSON files, each corresponding to the starting letter of the article, from letters A-Z, as well as files for articles starting with numbers and articles starting with anything else.

After each article was parsed, I had to create an "edge" to connect it to the first article it links to. This was done by creating yet another dictionary, this one linking the title of the article in question to the title of the first article it links to. This was by far the most tedious process; while the articles could be scraped in bundles between 50-200 in size, the edges have to be created by visiting each article *one at a time*, which means one server request per article, instead of one per batch of up to 200. However, since I had the list of articles already created (and sorted into separate files), I was able to use multiprocessing to parallelize the process. *Side note: it also let me see that over 25% of articles on Wikipedia start with either the letter A, C, M, or S, with S having the most articles!*

Since this had to be done over a long stretch of time, and I still have final assignments to complete for school, I needed to ensure there was a consistent way to stop and start this process. Initially, I tried placing the whole function in a try-except to catch the keyboard interrupt, but occassionally it would stop execution while one thread was mid-write. Eventually, the most consistent (and quickest) fix I could find was to turn off my computer's Internet connection, creating an HTTP connection error, which would cause each thread to write to file, and *then* use an interrupt. In addition, I kept a separate folder on-hand of each file's progress, that I would update before ending execution to make sure files didn't get overwritten with nothing (which did happen for the first few iterations).

## Pitfalls
There were various issues that I encountered trying to get the parser to work, as it's impossible to account for absolutely every edge case in a project like this.

My first issue was that some articles with valid, non-parenthesized links, wouldn't get picked up by the parser. This was a quick and easy fix, since previously the script would only iterate through `<p>` tags, while those links were embedded in `<li>` tags. As such, I took the main functionality of the parsing method and generalized it to deal with a batch of tags. That way, I could call it for both `<p>` tags, as well as `<li>` tags.

Another issue I ran into after finishing up the primary functionality was that a lot of articles had their first link set to the page for [Geographic coordinate system](https://en.wikipedia.org/wiki/Geographic_coordinate_system). After doing some investigation, I found that this was because, for Wikipedia pages about locations, a small paragraph tag is included at the very top right corner of the page, indicating the location's global coordinates. Fortunately, this was a quick fix. Since the tags contained a `<span>` with the ID "coordinates," I could just skip over any tag that had a child tag with this ID. Making use of my cleanup method, I was also able to remove the faulty connections by name, and running the main algorithm one more time (for about 15 minutes or so), I was able to refresh those connections.

In addition, some articles along the critical path to Philosophy (namely, articles for [Logic](https://en.wikipedia.org/wiki/Logic) and [Reason](https://en.wikipedia.org/wiki/Reason)) would point to the incorrect article [Religious philosophy](https://en.wikipedia.org/wiki/Religious_philosophy). Looking into this, I found that they were scraping these values from the infobox table, as it had a single paragraph tag inside of it that was appearing first in the hierarchy. Since I'm not looking for links in tables anyways, I simply removed all table tags before beginning parsing, which seemed to fix the issue. However, to go back and rescrape every article again would likely take another week, so I'll tweak these on a case by case basis as I test pages. Interestingly, this issue also seemed to affect some pages that contained presidential or otherwise notable signatures, as they would be contained in `<a>` tags, with a proper title, though that title would simply read "\[Person\]'s signature", and would cause an error when trying to traverse the data, as those pages do not exist.

## Functionality
Below are some of the functions and fun games I've made using this dataset.

### Getting to Philosophy
Contianed in `get_to_philosophy.py` are the necessary methods to enter in a Wikipedia article's title, and have a path found from that article to reach either Philosophy, or loop around somewhere. Most articles will eventually reach Philosophy, though there are some that will end up looping back in on themselves (interestingly, [Mathematics](https://en.wikipedia.org/wiki/Mathematics) (as of 12/15/22) loops in on itself, with its path going to [Number theory](https://en.wikipedia.org/wiki/Number_theory), [Pure mathematics](https://en.wikipedia.org/wiki/), and then back to Mathematics!). Thanks to all articles being stored in a dictionary, paths can be generated incredibly quickly, which means not waiting for dozens of requests to get back to get an answer.
